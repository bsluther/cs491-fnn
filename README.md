This is a ground-up (almost, we used Numpy) implementation of a multilayer feedforward neural network completed as a group project for CS 491, "Neural Networks", at the University of New Mexico in Fall 2024. My contributions included the [backpropagation](https://github.com/bsluther/cs491-fnn/blob/acea55614664bf7d4404efcbf46af5aa4b4940ab/fnn2.py#L153) implementation, and [supporting modules](https://github.com/bsluther/cs491-fnn/blob/acea55614664bf7d4404efcbf46af5aa4b4940ab/fnn2.py#L108), testing and hyperparameter tuning, and an [illustration](https://github.com/bsluther/cs491-fnn/blob/main/newton-illustration.png) of how the Newton method can be used during gradient descent. Trying to derive the correct formulas for the second-order derivatives used in the Newton method was an interesting exercise, I'm still not sure I got them right, although I was able to confirm them with a [concrete example](https://github.com/bsluther/cs491-fnn/blob/main/newton-method-computation.pdf). It would be fun to re-visit them with more experience and confirm or else find the errors in them. My understanding is that in general, the cost associated computing the second-order derivatives is too large to justify their increased accuracy, so the Newton method is not generally used for training neural networks.
